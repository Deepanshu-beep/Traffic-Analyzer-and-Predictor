{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the current directory with the Location Yolo Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Project\\Object Recog. with YOLO\n"
     ]
    }
   ],
   "source": [
    "cd F:\\Project\\Object Recog. with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='F:\\Project\\Object Recog. with YOLO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Threshold for Confidence filter Weak Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_COnfidence=0.5\n",
    "NMS_Thres=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Create a function to Detect Features we want, We can extract more features as well which are avilable in coco.names in YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFeatures(frame, net, ln, personIndx=0, bicycleIndx=0, carIndx=0, motorbikeIndx=0, busIndx=0):\n",
    "    #Getting Height, Width of the current frame\n",
    "    (H,W)=frame.shape[:2]\n",
    "    #We'll store out predicitons with it's details such as confidence, bounding box points in results\n",
    "    results=[]\n",
    "    #Creating a blob of current frame\n",
    "    blob=cv2.dnn.blobFromImage(frame, 1/255.0,(416,416), swapRB=True, crop=False)\n",
    "    #Giving the blob as input to our model to process this frame\n",
    "    net.setInput(blob)\n",
    "    #Getting the output of our Model which contains predictions\n",
    "    layerOutputs=net.forward(ln)\n",
    "    #We'll Stores the bounding box co-ordinates, confidences, ID which corresponds to object it is in these for our predictions\n",
    "    boxes=[]\n",
    "    confidences=[]\n",
    "    classIDs=[]\n",
    "    #Iterate though our outputs and each detection in each output\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            #Extract Confidence for each detection and it's ID for item it corresponds to\n",
    "            scores=detection[5:]\n",
    "            classID=np.argmax(scores)\n",
    "            confidence=scores[classID]\n",
    "            #We just want Persons, Bicycles, Cars, Bikes and Buses to be detected\n",
    "            if (classID==personIndx or classID==bicycleIndx or classID==carIndx or classID==motorbikeIndx or classID==busIndx) and confidence>min_COnfidence:\n",
    "                #Get the boundary box co-ordinates for our Detectection\n",
    "                box=detection[0:4]*np.array([W,H,W,H])\n",
    "                (centerX, centerY, width, height)=box.astype('int')\n",
    "                x=int(centerX-(width/2))\n",
    "                y=int(centerY-(height/2))\n",
    "                #Save these co-ordinates, confidence, ID of item number in the lists we initialized\n",
    "                boxes.append([x,y,int(width),int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "    #Applying Non-Max Suppression to remove extra Bounding Boxes\n",
    "    indxs=cv2.dnn.NMSBoxes(boxes, confidences, min_COnfidence, NMS_Thres)\n",
    "    #Storing Final Detection's features in results\n",
    "    if len(indxs)>0:\n",
    "        for i in indxs.flatten():\n",
    "            (x,y)=(boxes[i][0], boxes[i][1])\n",
    "            (w,h)=(boxes[i][2], boxes[i][3])\n",
    "            r=(confidences[i], (x,y,x+w,y+h),classIDs[i])\n",
    "            results.append(r)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imutils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsPath='F:\\Project\\Object Recog. with YOLO\\coco.names'\n",
    "labels=open(labelsPath).read().strip().split('\\n')\n",
    "weightPaths='F:\\Project\\Object Recog. with YOLO\\yolov3.weights'\n",
    "configPath='F:\\Project\\Object Recog. with YOLO\\yolov3.cfg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=cv2.dnn.readNetFromDarknet(configPath, weightPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Project\\Traffic Analyzer\n"
     ]
    }
   ],
   "source": [
    "cd F:\\Project\\Traffic Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving the Input Video to process and output Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp='inp2.mp4'\n",
    "output='output_01.avi'\n",
    "display=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln=net.getLayerNames()\n",
    "ln=[ln[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "vs=cv2.VideoCapture(inp)\n",
    "writer=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #Reading frame from video\n",
    "    (check, frame1) = vs.read()\n",
    "    if not check:\n",
    "        break\n",
    "    frame=frame1.copy()\n",
    "    #Resizing Frame\n",
    "    frame = imutils.resize(frame, width=700)\n",
    "    #Extracting Results from the above function\n",
    "    results = detectFeatures(frame, net, ln,personIndx=labels.index('person'), bicycleIndx=labels.index('bicycle'), carIndx=labels.index('car'), motorbikeIndx=labels.index('motorbike'), busIndx=labels.index('bus'))\n",
    "    #Setting Number of Vehicles initially 0\n",
    "    twoWheeler=0\n",
    "    fourWheeler=0\n",
    "    person=0\n",
    "    #Iterating through our results in Current frame\n",
    "    for (i,(prob, bbox, classID))in enumerate(results):\n",
    "        #Getting co-ordinates our current detection, and adding to corresponding Detection we got\n",
    "        (startX, startY, endX, endY)=bbox\n",
    "        if classID==labels.index('person'):\n",
    "            person+=1\n",
    "            color=(255,255,0)\n",
    "        elif classID==labels.index('car') or classID==labels.index('bus'):\n",
    "            fourWheeler+=1\n",
    "            color=(0,255,0)\n",
    "        elif classID==labels.index('bicycle') or classID==labels.index('motorbike'):\n",
    "            twoWheeler+=1\n",
    "            color=(100,100,100)\n",
    "        #Plotting rectangle around the detection and displaying what it is\n",
    "        cv2.rectangle(frame, (startX,startY),(endX,endY),color,1)\n",
    "        cv2.putText(frame, labels[classID], (startX,startY-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,color,1)\n",
    "    #Displaying Summary of traffic on top-left\n",
    "    text1='4-Wheelers: {}'.format(fourWheeler)\n",
    "    text2='2-Wheelers: {}'.format(twoWheeler)\n",
    "    text3='Pedestrians: {}'.format(person)\n",
    "    cv2.putText(frame, text1, (25,frame.shape[0]-350),cv2.FONT_HERSHEY_SIMPLEX,0.6, (0,0,255),2)\n",
    "    cv2.putText(frame, text2, (25,frame.shape[0]-325),cv2.FONT_HERSHEY_SIMPLEX,0.6, (0,0,255),2)\n",
    "    cv2.putText(frame, text3, (25,frame.shape[0]-300),cv2.FONT_HERSHEY_SIMPLEX,0.6, (0,0,255),2)\n",
    "    #If display>0, you'll get to see each frame while it's being processed and written to Output file\n",
    "    if display>0:\n",
    "        cv2.imshow('Frame',frame)\n",
    "        key=cv2.waitKey(0)\n",
    "        if key==27:\n",
    "            break\n",
    "        cv2.destroyAllWindows()\n",
    "    if output!='' and writer is None:\n",
    "        fourcc=cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        writer=cv2.VideoWriter(output, fourcc, 10, (frame.shape[1],frame.shape[0]))\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
